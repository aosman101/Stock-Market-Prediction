{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Stock Market Prediction Using LSTM Networks </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Import the gold.csv file\n",
    "df = pd.read_csv(r'Project Code/tesla.csv')\n",
    "\n",
    "# Display the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The standard deviation (std) in the `describe()` function quantifies the variation or dispersion within the data set. A higher standard deviation signifies that the \n",
    "\n",
    "data points are dispersed over a broader range of values. In contrast, a lower standard deviation indicates that the data points cluster closely around the mean. </p>\n",
    "\n",
    "\n",
    "<p> Standard Deviation for Key Columns\n",
    "\n",
    "- Open: The standard deviation is 110.838840, indicating that the opening prices exhibit considerable variability from the mean opening price of 84.068473.\n",
    "- High: The standard deviation is 113.321844, demonstrating that the highest prices throughout the day also show significant fluctuations around the mean of 85.916742.\n",
    "- Low: The standard deviation is 108.110895, suggesting that the lowest prices during the day are similarly spread around the mean of 82.091640.\n",
    "- Close: The standard deviation is 110.747440, indicating that the closing prices vary significantly around the mean closing price of 84.042719.\n",
    "\n",
    "Other Columns:\n",
    "- Volume: The standard deviation is 7.744906e+07, reflecting a substantial variation in trading volume compared to the mean of 9.651920e+07.\n",
    "- Dividends: The standard deviation is 0.0, which signifies no variation in dividend values, as all entries are uniformly 0.\n",
    "- Stock Splits: The standard deviation is 0.096083, indicating minimal variation around the mean of 0.002173.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data Cleaning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop = True, inplace = True) # Resetting the index.\n",
    "df.sort_values(by = 'Date', ascending = True, inplace = True) # Sorting the data by 'Date' in ascending order.\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors = 'coerce', utc = True) # Converting the 'Date' column to datetime.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Visualisation of the data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the closing price of the stock.\n",
    "plt.figure(figsize = (14, 7))\n",
    "plt.plot(df['Date'], df['Close'], label = 'Close Price')\n",
    "plt.title('Tesla Stock Closing Price') \n",
    "plt.xlabel('The Date') \n",
    "plt.ylabel('The Closing Price')\n",
    "plt.legend()\n",
    "plt.show() \n",
    "\n",
    "# visualise the volume of stocks sold each year\n",
    "plt.figure(figsize = (15, 6))\n",
    "df.set_index('Date')['Volume'].plot()\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Volume')\n",
    "plt.title(\"Sales Volume of Tesla\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and calculate average closing price per year\n",
    "df['Year'] = df['Date'].dt.year\n",
    "yearly_avg = df.groupby('Year')['Close'].mean()\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(yearly_avg.index, yearly_avg.values, marker = 'o', linestyle = '-', color = 'b', label = 'Average Yearly Close')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Closing Price')\n",
    "plt.title('Yearly Average Closing Prices')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Daily stock returns.\n",
    "df['Daily Return'] = df['Close'].pct_change()\n",
    "plt.figure(figsize= ( 12,6))\n",
    "plt.plot(df['Date'], df['Daily Return'], color = \"red\", label = \"Daily Returns\")\n",
    "plt.axhline(0, color = \"black\", linestyle = \"--\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Daily Return\")\n",
    "plt.title(\"Daily Stock Returns Over Time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Daily Return'].std()) # Standard deviation of the daily returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "The output 0.03614 represents the standard deviation of daily returns in the dataset. A value of around 0.036 indicates the volatility of these returns, with higher values signifying more significant fluctuations and lower values suggesting more stability. This metric is crucial for assessing risk, as it measures how much returns deviate from the mean, helping investors gauge the asset's potential unpredictability.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the moving averages.\n",
    "df['50-day MA'] = df['Close'].rolling(window = 50).mean()\n",
    "df['200-day MA'] = df['Close'].rolling(window = 200).mean()\n",
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(df['Date'], df['Close'], label = \"Closing Price\", alpha = 0.5)\n",
    "plt.plot(df['Date'], df['50-day MA'], label = \"50-day MA\", color = \"red\")\n",
    "plt.plot(df['Date'], df['200-day MA'], label = \"200-day MA\", color = \"green\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Stock Closing Price with Moving Averages\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Drop non-numeric columns.\n",
    "df_numeric = df.select_dtypes(include = ['number'])\n",
    "\n",
    "# Calculate the correlation matrix.\n",
    "corr_matrix = df_numeric.corr()\n",
    "\n",
    "# Plot the correlation heatmap.\n",
    "plt.figure(figsize = (8,6))\n",
    "sns.heatmap(corr_matrix, annot = True, cmap = 'coolwarm', fmt = \".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Price-related variables, namely Open, High, Low, and Close, exhibit a high correlation (approximately ~1.00). This is understandable, as these values typically move in tandem with stock prices.\n",
    "\n",
    "The 50-day moving average (MA) and 200-day MA show a strong correlation (around ~0.96-0.98) with stock prices. Moving averages serve to smooth price trends and inherently follow price movements. In contrast, trading volume demonstrates only a weak correlation (ranging from ~0.08 to 0.35) with price changes, indicating that volume does not significantly impact price fluctuations. Daily returns exhibit minimal correlation with other features (approximately ~-0.02 to 0.05), suggesting that stock returns fluctuate relatively independently of individual price points. \n",
    "\n",
    "Finally, the year variable reflects a moderate correlation (around ~0.80) with stock prices, which may indicate an upward trend over time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis.\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors = 'coerce', utc = True)  # Force UTC.\n",
    "df['Month'] = df['Date'].dt.month \n",
    "df['Month'] = df['Date'].dt.month\n",
    "monthly_avg = df.groupby('Month')['Close'].mean()\n",
    "monthly_avg.plot(kind = 'bar', title = \"Monthly Average Closing Price\", figsize = (10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The graph depicts the monthly average closing price, starting and finishing at 100, with significant fluctuations observed throughout the year. A downward trend begins in month two, gradually declining until it drops sharply in month five, representing the period's lowest point. This mid-year decline suggests a temporary market correction or the influence of external economic factors. Nevertheless, the recovery by the year's end demonstrates resilience and a return to stability. Overall, the observed pattern illustrates cyclical volatility, wherein prices are influenced by seasonal trends, market events, or investor behaviour, ultimately regaining strength by year-end.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Building the LSTM Model </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Train The Model on Close Price 80 - 20 Spilt   </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column to datetime and sort the data\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(by = 'Date')\n",
    "\n",
    "# Select only the 'Close' price\n",
    "close_model_df = df[['Date', 'Close']]\n",
    "\n",
    "# Normalize the 'Close' column\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "close_model_df['Close'] = scaler.fit_transform(close_model_df[['Close']])\n",
    "\n",
    "# Display first few rows\n",
    "close_model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])  # Last `seq_length` days\n",
    "        y.append(data[i + seq_length])   # Predict next day\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Convert Close column to NumPy array\n",
    "close_prices = close_model_df['Close'].values\n",
    "\n",
    "# Define sequence length (e.g., 50 days)\n",
    "seq_length = 50\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(close_prices, seq_length)\n",
    "\n",
    "# Split into training (80%) and testing (20%) sets\n",
    "split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "# Reshape for LSTM (samples, time steps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "The shapes of X_train and X_test are adjusted to meet the input requirements of the LSTM model, which expects a 3D input structure comprising samples, time steps, and features (e.g. the closing price). Specifically:\n",
    "\n",
    "- Samples denote the number of sequences.\n",
    "- Time steps represent the length of each sequence (e.g., 50 days).\n",
    "- Features indicate the number of variables at each time step (1 for univariate time series).\n",
    "\n",
    "This data restructuring ensures that the LSTM can effectively learn temporal dependencies.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "Layer Type | Purpose\n",
    "\n",
    "LSTM (50 units, return_sequences = True): This initial LSTM layer is designed to learn patterns from the past 50 days. We use `return_sequences = True` because another LSTM layer follows it.\n",
    "\n",
    "Dropout (0.2): This dropout layer randomly removes 20% of neurons to help prevent overfitting.\n",
    "\n",
    "LSTM (50 units, return_sequences = False): The second and final LSTM layer extracts meaningful features from the data. We set `return_sequences = False` since no additional LSTM layers follow.\n",
    "\n",
    "Dropout (0.2): Another dropout layer is included to mitigate overfitting.\n",
    "\n",
    "Dense (25 units, activation = 'relu'): This fully connected layer consists of 25 neurons, and it aims to transform the features derived from the LSTM layers into a more meaningful representation.\n",
    "\n",
    "Dense (1 unit): This is the output layer, which predicts the stock price for the next day.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences = True, input_shape = (50, 1)),  # LSTM layer with 50 units\n",
    "    Dropout(0.2),  # Dropout to prevent overfitting\n",
    "    LSTM(50, return_sequences = False),  # Another LSTM layer\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation = 'relu'),  # Dense layer with 25 neurons\n",
    "    Dense(1)  # Output layer (predict next close price)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mae'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "This output summarises the architecture for an LSTM-based sequential model created using Keras. The model features two LSTM layers, each containing 50 units, succeeded by dropout layers with a 20% dropout rate to mitigate overfitting. Following the LSTM layers, there are two dense (fully connected) layers: one with 25 neurons utilising ReLU activation and a final output layer with a single neuron designed to predict the next closing price.\n",
    "\n",
    "The parameter counts reflect the number of trainable weights within each layer:\n",
    "- The first LSTM layer has 10,400 parameters.\n",
    "- The second LSTM layer contains 20,200 parameters.\n",
    "- The dense layers have 1,275 and 26 parameters, respectively.\n",
    "\n",
    "Overall, the model encompasses 31,901 trainable parameters. The loss function employed is a mean squared error (MSE), and optimisation is carried out using the Adam optimiser. Mean absolute error (MAE) is tracked as a metric.\n",
    "\n",
    "The Adam optimiser, or Adaptive Moment Estimation, is preferred for training LSTM models due to its blend of efficiency, stability, and adaptability, which are essential for handling the complexities of LSTM networks. By combining the advantages of Momentum and RMSProp, Adam maintains moving averages of gradients and squared gradients, allowing it to adjust learning rates for each parameter individually. This leads to faster and more reliable convergence, particularly in models with complex error surfaces.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping to stop if validation loss doesn't improve for 5 epochs\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs = 50,  # We can set a high number, and early stopping will stop it earlier\n",
    "    batch_size = 32,\n",
    "    validation_data = (X_test, y_test),\n",
    "    callbacks = [early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "This output illustrates the training process of an LSTM model designed to predict stock closing prices. The model was trained for 50 epochs with an early stopping mechanism. This mechanism monitors the validation loss (val_loss) and halts training if there is no improvement over five consecutive epochs while restoring the best weights identified during training.\n",
    "\n",
    "Performance Details:\n",
    "- Loss (loss) refers to the model's error on the training dataset, which is measured using Mean Squared Error (MSE).\n",
    "- Validation Loss (val_loss) indicates the model's error on the unseen validation dataset using MSE. Lower values indicate better generalisation.\n",
    "- Mean Absolute Error (MAE), represented as mae and val_mae, reveals the average absolute difference between predicted and actual values, providing an intuitive understanding of prediction accuracy.\n",
    "\n",
    "Observations:\n",
    "In the early epochs (Epoch 1), the training loss begins at 0.0130, while the validation loss is at 0.0031, suggesting that the model is actively learning. As training advances, the loss values decrease noticeably. For instance, by Epoch 10, the training loss has diminished to approximately 5.01e-04, and the validation loss has reduced to 0.0014. \n",
    "\n",
    "In the later epochs, both training and validation losses stabilise. By Epochs 23-24, the training loss hovers around ~3.6e-04, stabilising the validation loss between ~0.0010 and 0.0012. This indicates that the model has converged and is neither overfitting nor underfitting.\n",
    "\n",
    "The LSTM model exhibits impressive performance, as indicated by its low training and validation losses, suggesting that it has successfully captured the underlying patterns of stock closing prices. The low validation MAE values (falling below 0.03) further indicate that the model produces relatively accurate predictions on the validation dataset. The incorporation of early stopping has effectively mitigated overfitting by halting training once the validation performance reached a plateau.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.plot(history.history['loss'], label = 'Training Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "The Training vs. Validation Loss graph shows how the model's Mean Squared Error (MSE) changes over epochs for both datasets. The blue line for Training Loss decreases smoothly and remains low, indicating a good fit to the training data. In contrast, the orange Validation Loss line fluctuates more, suggesting variability in generalisation to unseen data but generally trends downward—a positive sign. The small gap between training and validation losses indicates minimal overfitting. However, spikes in validation loss may suggest sensitivity to specific data points or slight overfitting in later epochs.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Reverse the normalization (convert back to actual price)\n",
    "y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_pred_actual = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# Plot actual vs predicted prices\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.plot(y_test_actual, label = 'Actual Prices', color = 'blue')\n",
    "plt.plot(y_pred_actual, label = 'Predicted Prices', color = 'red')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.title('Actual vs. Predicted Stock Prices')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "The predicted prices generated by the LSTM model (represented by the red line) closely align with the actual stock prices (illustrated by the blue line) over the entire period. These predictions effectively capture the overarching trend and the short-term fluctuations of the actual prices, showcasing the model's capability to learn intricate patterns within the data. The minimal difference between the predicted and actual prices reflects the high accuracy and reliability of the LSTM model in forecasting stock prices in this instance.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set.\n",
    "mse, mae = model.evaluate(X_test, y_test, verbose = 1)\n",
    "print(f\"Test MSE: {mse:.5f}\")\n",
    "print(f\"Test MAE: {mae:.5f}\")\n",
    "\n",
    "# Gives the error in the model.\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'RMSE: {rmse:.2f}')\n",
    "\n",
    "# How well does the model explain the variance.\n",
    "r2 = r2_score(y_test_actual, y_pred_actual)\n",
    "print(f'R² Score: {r2:.4f}')\n",
    "\n",
    "# Calculate the model accuracy.\n",
    "accuracy_percentage = r2 * 100\n",
    "print(f'Model Accuracy: {accuracy_percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "- Test MSE: 0.00155 — This value represents the Mean Squared Error on the test dataset, demonstrating a very low average squared difference between the predicted and actual closing prices, which indicates high prediction accuracy.  \n",
    "- Test MAE: 0.02975 — The Mean Absolute Error for the test data reveals that the model's average predictions deviate by approximately 0.02314 units (likely aligned with the scale used for the normalised or scaled stock prices), highlighting the model's strong performance.  \n",
    "\n",
    "- Root Mean Squared Error (RMSE): 0.04 - This indicates that, on average, predicted stock prices deviate from actual prices by only 0.03 units, reflecting high accuracy in the model's predictions.\n",
    "\n",
    "- R² Score (Coefficient of Determination): 0.9194 - This score signifies that the LSTM model explains 94.7% of the variance in stock prices, demonstrating an excellent fit and the ability to identify patterns from historical data.\n",
    "\n",
    "- Model Accuracy: 91.94% - Regression models typically do not use accuracy percentages, but this score highlights the model's reliability in explaining variance.\n",
    "\n",
    "In summary, the low RMSE, high R² score, and substantial accuracy collectively emphasize the LSTM network's effectiveness in forecasting stock prices. The model shows strong performance on training and test datasets, with a validation curve that aligns with the training curve and a minimal Mean Absolute Error (MAE), indicating accurate predictions of unseen stock price data.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Train The Model on Close & Volume 80 - 20 Spilt </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the 'Close' price\n",
    "close_volume_model_df = df[['Date', 'Close', 'Volume']]\n",
    "\n",
    "# Normalize 'Close' and 'Volume' separately\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "close_volume_model_df[['Close', 'Volume']] = scaler.fit_transform(close_volume_model_df[['Close', 'Volume']])\n",
    "\n",
    "# Display first few rows\n",
    "close_volume_model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences with multiple features\n",
    "def create_sequences_multifeature(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])  # Use both 'Close' and 'Volume'\n",
    "        y.append(data[i + seq_length, 0])  # Predict only 'Close' price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Convert dataframe to NumPy array\n",
    "data_values = close_volume_model_df[['Close', 'Volume']].values\n",
    "\n",
    "# Define sequence length (e.g., 50 days)\n",
    "seq_length = 50\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences_multifeature(data_values, seq_length)\n",
    "\n",
    "# Split into training (80%) and testing (20%) sets\n",
    "split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "# Reshape for LSTM (samples, time steps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 2))  # 2 features\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 2))\n",
    "\n",
    "# Print shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "The shapes of X_train and X_test are reshaped to (samples, time steps, and features) to meet the input requirements of the LSTM model for multivariate time series data. In this context, two features (such as 'Close' price and 'Volume') are utilised, resulting in a shape that represents:\n",
    "\n",
    "- Samples: The number of sequences,\n",
    "- Time steps: The length of each sequence (for instance, 50 days),\n",
    "- Features: The number of variables at each time step (2).\n",
    "\n",
    "This configuration enables the LSTM to learn from multiple features concurrently, thereby capturing more nuanced temporal patterns to enhance prediction accuracy.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model with multiple features\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(seq_length, 2)),  # 2 features now\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1)  # Predicting 'Close' price only\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "The model architecture consists of LSTM layers. The first LSTM layer contains 50 units and is configured with `return_sequences = True`. It accepts an input shape of `(seq_length, 2)`. The second LSTM layer has 50 units but is set to `return_sequences = False`, producing a single output vector. Two dropout layers with a 20% dropout rate are added after each LSTM layer to reduce overfitting.\n",
    "\n",
    "Following the LSTM layers, there is a dense layer with 25 units and ReLU activation, which feeds into a final layer with a single unit to predict the 'Close' price. The model utilises the Adam Optimiser and employs Mean Squared Error (MSE) as the loss function, with Mean Absolute Error (MAE) as a metric for accuracy. The first LSTM layer has 10,600 parameters, the second has 20,200, and the dense layers have 1,275 and 26 parameters, resulting in 32,101 trainable parameters. This model aims to capture temporal dependencies in the data to improve stock price predictions. The dropout layers help promote generalisation, while the dense layers enhance accuracy. \n",
    "\n",
    "Overall, the architecture is designed for effective time-series forecasting by balancing complexity and performance.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping to stop if validation loss doesn't improve for 5 epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs = 50,\n",
    "    batch_size = 32,\n",
    "    validation_data = (X_test, y_test),\n",
    "    callbacks = [early_stopping],\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training vs. validation loss.\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.plot(history.history['loss'], label = 'Training Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "The \"Training vs. Validation Loss\" graph illustrates the LSTM model's performance regarding stock closing prices and volume across training epochs. The blue line represents Training Loss, while the orange line denotes Validation Loss. Both metrics show a significant decrease early on, indicating effective learning. The convergence of the two lines suggests good generalization without overfitting, and the stable validation loss of around 0.002 demonstrates the model's substantial predictive accuracy for stock prices, confirming the LSTM's ability to capture underlying data patterns.\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Reverse normalization\n",
    "y_test_actual = scaler.inverse_transform(np.column_stack((y_test, np.zeros_like(y_test))))[:, 0]\n",
    "y_pred_actual = scaler.inverse_transform(np.column_stack((y_pred.flatten(), np.zeros_like(y_pred.flatten()))))[:, 0]\n",
    "\n",
    "# Plot actual vs. predicted prices\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.plot(y_test_actual, label = 'Actual Prices', color = 'blue')\n",
    "plt.plot(y_pred_actual, label = 'Predicted Prices', color = 'red')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.title('Actual vs. Predicted Stock Prices')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "The graph titled \"Actual vs Predicted Stock Prices\" illustrates a comparison between the predictions made by the LSTM model (represented by the red line) and the actual stock prices (shown by the blue line). The predicted prices closely align with the actual values, showcasing the model's impressive ability to capture trends and patterns within the data. Although there are minor discrepancies, the overall correspondence reflects a high level of accuracy in forecasting. This performance underscores the effectiveness of the LSTM model in utilising historical price and volume data for reliable predictions, establishing it as a valuable asset for stock price analysis.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model.\n",
    "mse, mae = model.evaluate(X_test, y_test, verbose = 1)\n",
    "print(f\"Test MSE: {mse:.5f}\")\n",
    "print(f\"Test MAE: {mae:.5f}\")\n",
    "\n",
    "# Gives the error in the model.\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'RMSE: {rmse:.2f}')\n",
    "\n",
    "# How well does the model explains the variance.\n",
    "r2 = r2_score(y_test_actual, y_pred_actual)\n",
    "print(f'R² Score: {r2:.4f}')\n",
    "\n",
    "# Calculate the model accuracy.\n",
    "accuracy_percentage = r2 * 100\n",
    "print(f'Model Accuracy: {accuracy_percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "The results display the LSTM model's performance on the test set, revealing a Test MSE of 0.00105 and a Test MAE of 0.02379. The low MSE reflects minimal squared prediction errors, while the low MAE indicates that the model's predictions closely align with the actual stock prices on average. These findings underscore the model's high accuracy and reliability in forecasting stock prices.\n",
    "\n",
    "Root Mean Squared Error (RMSE): 0.03 - This indicates that, on average, predicted stock prices deviate from actual prices by only 0.03 units, reflecting high accuracy in the model's predictions. R² Score (Coefficient of Determination): 0.9453 - This score signifies that the LSTM model explains 94.7% of the variance in stock prices, demonstrating an excellent fit and the ability to identify patterns from historical data. Model Accuracy: 94.53% - Regression models typically do not use accuracy percentages, but this score highlights the model's reliability in explaining variance.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Comparison against ARIMA Model </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Close' column to a numpy array.\n",
    "close_prices = df['Close'].values\n",
    "\n",
    "# Define training data length (80% of the data).\n",
    "training_data_len = int(len(close_prices) * 0.8)\n",
    "\n",
    "# Split the data into training and testing sets.\n",
    "train_data = close_prices[:training_data_len]\n",
    "test_data = close_prices[training_data_len:]\n",
    "\n",
    "# Fit the ARIMA model on the training data.\n",
    "arima_model = ARIMA(train_data, order = (5, 1, 0))\n",
    "arima_model_fit = arima_model.fit()\n",
    "\n",
    "# Make predictions for the test set period.\n",
    "start_index = training_data_len\n",
    "end_index = len(close_prices) - 1\n",
    "\n",
    "arima_predictions = arima_model_fit.predict(start = start_index, end = end_index, typ = 'levels')\n",
    "\n",
    "# Align predictions to match y_test length (if necessary).\n",
    "aligned_arima_predictions = arima_predictions[:len(y_test)]\n",
    "\n",
    "# Evaluate the ARIMA model using MSE and MAE for consistency.\n",
    "mse = np.mean((aligned_arima_predictions - y_test) ** 2)\n",
    "mae = np.mean(np.abs(aligned_arima_predictions - y_test))\n",
    "\n",
    "# Print the MSE and MAE.\n",
    "print(f\"Test MSE: {mse:.5f}\")\n",
    "print(f\"Test MAE: {mae:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "The ARIMA model shows a high Test MSE of 81273.41844 and a Test MAE of 285.08490, indicating poor predictive performance compared to the LSTM model, which has much lower errors with a Test MSE of 0.00074 and a Test MAE of 0.02051. ARIMA's linear design limits its ability to capture complex patterns and non-linear relationships in the data. In contrast, the LSTM model excels in accuracy and robustness by effectively learning sequential dependencies and non-linear trends. This comparison highlights the limitations of traditional models like ARIMA and the advantages of advanced approaches like LSTMs in predictive analytics.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with ARIMA.\n",
    "arima_predictions = arima_model_fit.predict(start = training_data_len, end = len(close_prices) - 1, typ = 'levels')\n",
    "\n",
    "# Align the lengths of arima_predictions and y_test.\n",
    "aligned_arima_predictions = arima_predictions[:len(y_test)]\n",
    "\n",
    "# Reverse normalisation for y_test (actual prices).\n",
    "# Assuming y_test is a 1D array and scaler was fit on the entire dataset.\n",
    "y_test_reshaped = y_test.reshape(-1, 1)\n",
    "y_test_actual = scaler.inverse_transform(np.hstack((y_test_reshaped, np.zeros_like(y_test_reshaped))))[:, 0]\n",
    "\n",
    "# Reverse normalisation for ARIMA predictions.\n",
    "arima_predictions_reshaped = aligned_arima_predictions.reshape(-1, 1)\n",
    "arima_predictions_actual = scaler.inverse_transform(np.hstack((arima_predictions_reshaped, np.zeros_like(arima_predictions_reshaped))))[:, 0]\n",
    "\n",
    "# Plot the actual vs. ARIMA predicted prices.\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.plot(y_test_actual, label = 'Actual Prices', color = 'blue')\n",
    "plt.plot(arima_predictions_actual, label = 'ARIMA Predictions', color = 'red')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.title('Actual vs. ARIMA Predicted Stock Prices')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "The analysis shows a significant gap between actual stock prices and those predicted by the ARIMA model, which struggles due to its linearity. ARIMA typically forecasts trends based on historical data but fails to capture sudden shifts or periods of stability, resulting in unrealistic constant predictions. While actual prices fluctuate realistically over time, ARIMA predictions remain flat at around 138,000, indicating an inability to recognize underlying patterns. This limitation stems from the model's assumptions of linearity and stationarity, making it unsuitable for the non-linear, volatile nature of financial time series. A more effective approach would be to use LSTM models or other machine learning techniques for better forecasting.\n",
    "\n",
    "Comparison against the LSTM Model predictions\n",
    "\n",
    "The predicted stock prices from the LSTM model closely align with the actual prices, showcasing the model's proficiency in capturing complex trends and fluctuations in the data. In contrast, the predictions from the ARIMA model significantly diverge from the actual values, with forecasts remaining unrealistically elevated and disconnected from reality. This stark contrast underlines the superior performance of the LSTM model over the ARIMA model in this instance, making it a more suitable choice for the dataset.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print ARIMA model summary.\n",
    "print(arima_model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "1. Model Overview\n",
    "   - ARIMA(5, 1, 0): This model features five autoregressive (AR) terms, one order of differencing (I), and no moving average (MA) terms.\n",
    "   - Log Likelihood: -7999.619, reflecting the model’s fit to the data (higher values signify better fit).\n",
    "   - AIC/BIC/HQIC: These are information criteria used for model comparison, where lower values indicate a preferable model. The AIC is 16011.238, BIC is 16047.163, and HQIC is 16024.173.\n",
    "\n",
    "2. Coefficients\n",
    "   - AR Terms: The coefficients for the AR terms (ar.L1 to ar.L5) illustrate their influence on the model. For instance, ar.L1 has a coefficient of -0.0274, suggesting a negative relationship with its prior value.\n",
    "   - Significance: P-values (P>|z|) denote the significance of each term. Terms with p-values < 0.05, such as ar.L1, ar.L2, ar.L3, and ar.L5, are statistically significant.\n",
    "\n",
    "3. Residuals\n",
    "   - Sigma2: The variance of the residuals is calculated to be 13.4186.\n",
    "   - Ljung-Box Test: A p-value of 0.97 indicates no significant autocorrelation in the residuals, which is favourable.\n",
    "   - Jarque-Bera Test: A p-value of 0.00 suggests that the residuals do not follow a normal distribution, which could be a concern.\n",
    "   - Heteroskedasticity: A p-value of 0.00 indicates the presence of heteroskedasticity, suggesting that the variance of residuals is not constant.\n",
    "\n",
    "4. Diagnostics\n",
    "   - Skewness and Kurtosis: The residuals are markedly non-normal with a skewness of 0.42 and a kurtosis of 45.65.\n",
    "\n",
    "5. Warnings\n",
    "   - The covariance matrix is estimated using the outer product of gradients, which is a standard method but may not be the most optimal in every situation.\n",
    "\n",
    "In summary, the ARIMA(5, 1, 0) model effectively captures some significant autoregressive patterns; however, it exhibits residual normality and heteroskedasticity issues that could impact its predictive capabilities. Further refinement or exploration of alternative models may be advantageous.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-sample predictions (on training data).\n",
    "arima_in_sample_predictions = arima_model_fit.predict(start = 0, end = training_data_len - 1, typ = 'levels')\n",
    "\n",
    "# Calculate errors (Mean Squared Error).\n",
    "in_sample_mse = np.mean((arima_in_sample_predictions - close_prices[:training_data_len]) ** 2)\n",
    "out_sample_mse = np.mean((aligned_arima_predictions - y_test_actual) ** 2)\n",
    "\n",
    "# Create lists for plotting.\n",
    "epochs = [1, 2]  # Only two points: In-sample (train) and Out-of-sample (val).\n",
    "training_mse_values = [in_sample_mse, in_sample_mse]  # Training MSE (constant for visualisation).\n",
    "validation_mse_values = [in_sample_mse, out_sample_mse]  # Validation MSE (starts at training MSE, ends at validation MSE).\n",
    "\n",
    "# Plot \"Training vs. Validation Loss\" for ARIMA.\n",
    "plt.figure(figsize = (8, 5))\n",
    "plt.plot(epochs, training_mse_values, label = 'Training MSE', marker = 'o', linestyle = '-', color = 'blue')\n",
    "plt.plot(epochs, validation_mse_values, label = 'Validation MSE', marker = 'o', linestyle = '-', color = 'red')\n",
    "plt.xticks([1, 2], ['Epoch 1', 'Epoch 2'])  # Label epochs for clarity.\n",
    "plt.ylabel('MSE')\n",
    "plt.title('ARIMA Training vs. Validation Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "The graph titled \"ARIMA Training vs. Validation Loss (MSE)\" illustrates the Mean Squared Error (MSE) for both the training and validation datasets over two epochs. An epoch is a complete pass through the entire training dataset in a machine learning model's training. It signifies one complete cycle during which the model has had the opportunity to observe and learn from all the training data at least once. The number of epochs is a hyperparameter that can impact the training duration and the model's performance.\n",
    "\n",
    "The Training MSE, represented by the blue line, remains stable, indicating the model's consistent performance on the training data. The Validation MSE, shown by the red line, begins at the same level as the Training MSE in Epoch 1 but decreases significantly in Epoch 2, suggesting improved generalization to unseen data. However, the widening gap between Training and Validation MSE may indicate potential overfitting, significantly if Validation MSE rises in future epochs. Ongoing monitoring is essential to maintain performance.\n",
    "\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
